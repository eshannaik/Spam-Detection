# -*- coding: utf-8 -*-
"""Spam Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXAjQ18b2ldnjzCle2ZIsdegnSImVcWa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.layers import LSTM,Dropout,Dense,Embedding
from keras.models import Sequential
from sklearn.model_selection import train_test_split
import seaborn as sns
from wordcloud import WordCloud
from sklearn.preprocessing import LabelEncoder
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score,confusion_matrix

nltk.download('punkt')

d=pd.read_csv('spam.csv', encoding='latin-1')

d.head()

df = d.iloc[:,0:2].values

df = pd.DataFrame(df)
df.columns=['Class','Text']

df.head()

df.info()

df.isna().sum()

df.describe()

y.shape

ns = df["Class"].isin(['ham']).sum(axis=0)
s = df["Class"].isin(['spam']).sum(axis=0)

label=['Spam','Not Spam']
a = [s,ns]
plt.pie(x=a,labels=label,autopct='%1.1f%%')
plt.legend()
plt.show()

plt.figure(figsize=(12,8))
ax =sns.barplot(x=label,y=a)
plt.title('Comparing number of spam messages to number of non spam messages')
for p in ax.patches:
    width, height = p.get_width(), p.get_height()
    x, y = p.get_xy() 
    ax.annotate('{}'.format(height), (x +0.25, y + height + 0.8))
plt.show()

def clean_data(text):
    out = re.sub('[^a-zA-Z]', ' ', text) 
    out = out.lower() 
    out = out.split()
    out = ' '.join(out)
    return out

def tokenize_word(text):
    return nltk.word_tokenize(text)

def remove_stopwords(text):
    stop_words = set(stopwords.words("english")+['u','ur','r','n']) 
    filtered_text = [word for word in text if word not in stop_words]
    return filtered_text

def lemmatize_word(text):
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]
    return lemmas

def get_processed_tokens(text):
    text = clean_data(text)
    text = tokenize_word(text)
    text = remove_stopwords(text)
    text = lemmatize_word(text)
    return text

nltk.download('stopwords')

nltk.download('wordnet')

df['processed_text'] = df['Text'].apply(get_processed_tokens)

df.head()

corpus= []
for i in df["processed_text"]:
    msg = ' '.join([row for row in i])
    corpus.append(msg)

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(corpus).toarray()
X.shape

text = ' '.join(corpus)
wc = WordCloud(background_color='black').generate(text)

plt.figure(figsize=[15,20])
plt.title("WORD CLOUD")
plt.axis("off")
plt.imshow(wc,interpolation='bilinear')

spam_corpus=[]
for i in df[df['Class']=='spam']["processed_text"]:
    msg = ' '.join([row for row in i])
    spam_corpus.append(msg)

text1 = ' '.join(spam_corpus)
wc = WordCloud(background_color='black').generate(text1)

plt.figure(figsize=[15,20])
plt.title("SPAM WORD CLOUD")
plt.axis("off")
plt.imshow(wc,interpolation='bilinear')

not_spam_corpus=[]
for i in df[df['Class']=='ham']["processed_text"]:
    msg = ' '.join([row for row in i])
    not_spam_corpus.append(msg)

text2 = ' '.join(not_spam_corpus)
wc = WordCloud(background_color='black').generate(text2)

plt.figure(figsize=[15,20])
plt.title("NOT SPAM WORD CLOUD")
plt.axis("off")
plt.imshow(wc,interpolation='bilinear')

"""### RNN"""

y = df.iloc[:,0:1]

print(y)

le=LabelEncoder()
y=le.fit_transform(np.ravel(y))

print(y)

X.shape

y.shape

x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=42)

x_train1 = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))
x_test1 = np.reshape(x_test, (x_test.shape[0],x_train.shape[1],1))
y_train1 = np.reshape(y_train, (y_train.shape[0],1,1))
y_test1 = np.reshape(y_test, (y_test.shape[0],1,1))

X_train1 = X_train1.astype('float32')
X_test1 = X_test1.astype('float32')

y_train1.shape

model=Sequential()

model.add(Embedding(30,30))
model.add(LSTM(50,recurrent_activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1,activation='softmax'))

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(x_train1,y_train1,epochs=3,batch_size=64,validation_data=(x_test1,y_test1))

model.summary()

score = model.evaluate(x_test, y_test, verbose=1)
ac_lstm=score[1]*100
print("Test Accuracy:", score[1])

"""### Dummy Classifier"""

from sklearn.dummy import DummyClassifier
dc = DummyClassifier(strategy='prior')
dc.fit(x_train,y_train)
y_dc=dc.predict(x_test)

cm_dc = confusion_matrix(y_test,y_dc)
sns.heatmap(cm_dc,annot=True,robust=True)

ac_dc = accuracy_score(y_test,y_dc) * 100
print(accuracy_score(y_test,y_dc))

"""### LightGBM"""

from lightgbm import LGBMClassifier
lgbm = LGBMClassifier(boosting_type='goss')
lgbm.fit(x_train,y_train)
y_lgbm = lgbm.predict(x_test)

cm_lgbm = confusion_matrix(y_test,y_lgbm)
sns.heatmap(cm_lgbm,annot=True,robust=True)

ac_lgbm = accuracy_score(y_test,y_lgbm) * 100
print(accuracy_score(y_test,y_lgbm))

"""### Multinomial Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB(fit_prior=True)
nb.fit(x_train,y_train)
y_nb = nb.predict(x_test)

cm_nb = confusion_matrix(y_test,y_nb)
sns.heatmap(cm_nb,annot=True,robust=True)

ac_nb = accuracy_score(y_test,y_nb) * 100
print(accuracy_score(y_test,y_nb))

"""### Naive Bayes"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)
y_knn=knn.predict(x_test)

cm_knn = confusion_matrix(y_test,y_knn)
sns.heatmap(cm_knn,annot=True,fmt='.2f')

ac_knn = accuracy_score(y_test,y_knn) * 100
print(accuracy_score(y_test,y_knn))

"""### Visualizing and Comparing the Accuracy Scores of the different Models"""

ac = [ac_dc,ac_lgbm,ac_nb,ac_knn,ac_lstm]
label = ['Dummy Classifier','LightGBM','Naive Bayes','K Nearest Neighbours','Long Short Term Memory']

plt.figure(figsize=(12,8))

ax = sns.barplot(x=label,y=ac)
plt.title('r2 score comparison among different regression models',fontweight='bold')

for p in ax.patches:
  width=p.get_width()
  height=p.get_height()
  x,y = p.get_xy()
  ax.annotate('{:.3f}%'.format(height), (x+0.25, y+height+0.8))

plt.show()